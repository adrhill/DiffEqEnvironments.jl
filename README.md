# DiffEqEnvironments.jl

| **Documentation**                                                               | **Build Status**      | **Code Coverage**               |
|:-------------------------------------------------------------------------------:|:---------------------:|:-------------------------------:|
| [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url] | [![][ci-img]][ci-url] | [![][codecov-img]][codecov-url] |

DiffEqEnvironments.jl bridges the gap between reinforcement learning and control theory by providing an easy way to **turn differential equations into RL environments**. Since it uses interfaces from [ReinforcementLearning.jl](https://juliareinforcementlearning.org), common algorithms from [ReinforcementLearningZoo.jl](https://github.com/JuliaReinforcementLearning/ReinforcementLearningZoo.jl) can be run and benchmarked on ODEs in a matter of minutes!


[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg
[docs-stable-url]: https://adrhill.github.io/DiffEqEnvironments.jl/stable

[docs-latest-img]: https://img.shields.io/badge/docs-dev-blue.svg
[docs-latest-url]: https://adrhill.github.io/DiffEqEnvironments.jl/dev

[ci-img]: https://github.com/adrhill/DiffEqEnvironments.jl/workflows/CI/badge.svg
[ci-url]: https://github.com/adrhill/DiffEqEnvironments.jl/actions

[codecov-img]: https://codecov.io/gh/adrhill/DiffEqEnvironments.jl/branch/master/graph/badge.svg
[codecov-url]: https://codecov.io/gh/adrhill/DiffEqEnvironments.jl